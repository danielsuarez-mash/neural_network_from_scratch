{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3004,"databundleVersionId":861823,"sourceType":"competition"}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/jollychappies/neural-network-from-scratch-using-numpy?scriptVersionId=185799806\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"bde4861d-2939-449e-ba57-954c3a432086","_cell_guid":"a26fad5f-f1af-444e-ad63-1f3a5d532d42","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-27T19:19:27.04381Z","iopub.execute_input":"2024-06-27T19:19:27.044094Z","iopub.status.idle":"2024-06-27T19:19:28.072473Z","shell.execute_reply.started":"2024-06-27T19:19:27.044071Z","shell.execute_reply":"2024-06-27T19:19:28.0712Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/digit-recognizer/sample_submission.csv\n/kaggle/input/digit-recognizer/train.csv\n/kaggle/input/digit-recognizer/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"# read csv\ntrain = pd.read_csv('/kaggle/input/digit-recognizer/train.csv')\ntest = pd.read_csv('/kaggle/input/digit-recognizer/test.csv')\n\n# get feature data\nX_train = train.drop(columns=['label']).transpose()\n\n# get target variables\ny_train = train['label']\n\n# convert to numpy arrays\ntrain = train.to_numpy()\nX_train = X_train.to_numpy()\ny_train = y_train.to_numpy()\ntest = test.to_numpy().transpose()","metadata":{"_uuid":"b68ecb2f-6d91-4860-98a2-33171f86bfbf","_cell_guid":"9990336a-5f62-4429-a4bf-d029862cbf86","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-27T19:19:28.074452Z","iopub.execute_input":"2024-06-27T19:19:28.074944Z","iopub.status.idle":"2024-06-27T19:19:33.9069Z","shell.execute_reply.started":"2024-06-27T19:19:28.07491Z","shell.execute_reply":"2024-06-27T19:19:33.906131Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"After recently completing Andrew Ng's course on Neural Networks and Deep Learning, I thought I'd implement what I've learned for this competition.","metadata":{"_uuid":"c233e976-0337-45d8-9075-163c42e4cdd8","_cell_guid":"e63f5b9c-9943-49d1-b533-68c53277d987","trusted":true}},{"cell_type":"markdown","source":"# Neural network from scratch.\n\nThe steps required to build and train a neural network are:\n\n1. Define layer architecture\n2. Initialise parameters\n3. Forward propagation\n4. Compute cost\n5. Backward propagation\n6. Update parameters","metadata":{"_uuid":"c2d27908-bf34-46e4-88da-639276ed3b28","_cell_guid":"7b0e8fd6-ae06-4ed8-8074-19b06ac614bf","trusted":true}},{"cell_type":"markdown","source":"# 1. Define layer architecture\n\nLet's find out how large our input layer must be from the number of pixels in an image.","metadata":{"_uuid":"f96f6124-d7f6-4518-baa9-b5c85a9cb747","_cell_guid":"f543a853-c177-4f32-a8ef-3b17c5d13b17","trusted":true}},{"cell_type":"code","source":"X_train.shape[0]","metadata":{"_uuid":"30d78262-3d89-44b3-921c-59b5e66b8d60","_cell_guid":"7b4f45e5-cfe4-4d93-9bff-d35da245cf1a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-27T19:19:33.90795Z","iopub.execute_input":"2024-06-27T19:19:33.908221Z","iopub.status.idle":"2024-06-27T19:19:33.914631Z","shell.execute_reply.started":"2024-06-27T19:19:33.908197Z","shell.execute_reply":"2024-06-27T19:19:33.9138Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"784"},"metadata":{}}]},{"cell_type":"code","source":"layer_dims = [784, 20, 15, 10]","metadata":{"_uuid":"53de733d-fbf4-4c3e-b217-a208d1bffd85","_cell_guid":"531ec1de-6de1-4d8b-8cd7-cbeaad9ddb54","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-27T19:19:33.917396Z","iopub.execute_input":"2024-06-27T19:19:33.917786Z","iopub.status.idle":"2024-06-27T19:19:33.924222Z","shell.execute_reply.started":"2024-06-27T19:19:33.917756Z","shell.execute_reply":"2024-06-27T19:19:33.923313Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# 2. Initialise parameters","metadata":{"_uuid":"bb30d5fb-a695-4db8-a63a-87c7726cc5ba","_cell_guid":"d09f9b0e-a35d-4b47-ae0c-56f3c1343843","trusted":true}},{"cell_type":"code","source":"def initialise_parameters(layer_dims):\n    \n    # number of layers\n    L = len(layer_dims)\n    \n    # initialise params dictionary\n    parameters = dict()\n    \n    # cycle through layers and initialise parameters\n    for l in range(1, L):\n        parameters['W'+str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1])*0.01\n        parameters['b'+str(l)] = np.zeros((layer_dims[l], 1))\n    \n    return parameters\n\nparameters = initialise_parameters(layer_dims)","metadata":{"_uuid":"560c84f8-61f6-4eec-8264-ad515f75cc2d","_cell_guid":"5154af37-410b-48a0-9fb3-930944c3297b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-27T19:19:33.92539Z","iopub.execute_input":"2024-06-27T19:19:33.926199Z","iopub.status.idle":"2024-06-27T19:19:33.934486Z","shell.execute_reply.started":"2024-06-27T19:19:33.92617Z","shell.execute_reply":"2024-06-27T19:19:33.933776Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# 3. Forward propagation","metadata":{}},{"cell_type":"code","source":"def linear_forward(A, W, b):\n    \n    Z = np.dot(W, A) + b\n\n    cache = (A, W, b)\n    return Z, cache","metadata":{"_uuid":"9f13e4d8-c3a5-46cd-85d0-ece917ba7aed","_cell_guid":"d713d5c1-4e11-450a-ab88-5d622fdbf7e8","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-27T19:19:33.935503Z","iopub.execute_input":"2024-06-27T19:19:33.935814Z","iopub.status.idle":"2024-06-27T19:19:33.944076Z","shell.execute_reply.started":"2024-06-27T19:19:33.935785Z","shell.execute_reply":"2024-06-27T19:19:33.943256Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def ReLU(Z):\n    \n    relu = np.maximum(0, Z)\n    \n    return relu","metadata":{"_uuid":"c4049d36-362d-4d37-945f-6b82f4e0c8c1","_cell_guid":"f5130888-cd98-443d-a753-f31a0b2b5e6f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-27T19:19:33.945205Z","iopub.execute_input":"2024-06-27T19:19:33.945723Z","iopub.status.idle":"2024-06-27T19:19:33.95352Z","shell.execute_reply.started":"2024-06-27T19:19:33.945694Z","shell.execute_reply":"2024-06-27T19:19:33.952794Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def sigmoid(Z):\n    \n    sigmoid = np.divide(1, 1+np.exp(-Z))\n    \n    return sigmoid","metadata":{"_uuid":"4baa629e-3f93-4ae4-863c-db7c75ef5c21","_cell_guid":"fbc7b2da-d22d-431d-8e9d-7d290deac4b7","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-27T19:19:33.954582Z","iopub.execute_input":"2024-06-27T19:19:33.954875Z","iopub.status.idle":"2024-06-27T19:19:33.96597Z","shell.execute_reply.started":"2024-06-27T19:19:33.954853Z","shell.execute_reply":"2024-06-27T19:19:33.965181Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# def softmax(Z):\n    \n#     softmax = np.exp(Z)/np.sum(np.sum(Z))\n    \n#     return softmax\n\ndef softmax(Z):\n    expZ = np.exp(Z - np.max(Z))\n    return expZ / expZ.sum(axis=0, keepdims=True)","metadata":{"_uuid":"8858bec3-8a06-4415-85dc-07e22ca2cfbc","_cell_guid":"f4cf46b4-6b26-4afa-98a3-4e5a02ed674a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-27T19:19:33.966836Z","iopub.execute_input":"2024-06-27T19:19:33.967071Z","iopub.status.idle":"2024-06-27T19:19:33.975261Z","shell.execute_reply.started":"2024-06-27T19:19:33.967042Z","shell.execute_reply":"2024-06-27T19:19:33.974439Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def linear_activation_forward(A_prev, W, b, activation):\n    \n    # compute Z\n    Z, linear_cache = linear_forward(A_prev, W, b)\n        \n    # compute A\n    if activation == 'relu':\n        A = ReLU(Z)\n    elif activation == 'sigmoid':\n        A = sigmoid(Z)\n    elif activation == 'softmax':\n        A = softmax(Z)\n    \n    # caches\n    activation_cache = Z\n    cache = (linear_cache, activation_cache)\n    \n    return A, cache","metadata":{"_uuid":"43a89086-8f5b-4527-8f88-a67aa8947704","_cell_guid":"59d4eeb2-ac31-4632-a9f7-2419b8416f01","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-27T19:19:33.978609Z","iopub.execute_input":"2024-06-27T19:19:33.9789Z","iopub.status.idle":"2024-06-27T19:19:33.98484Z","shell.execute_reply.started":"2024-06-27T19:19:33.978878Z","shell.execute_reply":"2024-06-27T19:19:33.983901Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# testing\nparameters = initialise_parameters(layer_dims)\nA, cache = linear_activation_forward(X_train, parameters['W1'], parameters['b1'], activation='softmax')","metadata":{"_uuid":"034bc7bd-2d94-425d-9abe-4885135247ae","_cell_guid":"83b71c3c-e5e9-42d1-86b0-e0a6d563511b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-27T19:19:33.985887Z","iopub.execute_input":"2024-06-27T19:19:33.986198Z","iopub.status.idle":"2024-06-27T19:19:34.121785Z","shell.execute_reply.started":"2024-06-27T19:19:33.986169Z","shell.execute_reply":"2024-06-27T19:19:34.120279Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def l_model_forward(X, parameters):\n    \"\"\"\n    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n    \n    Arguments:\n    X -- data, numpy array of shape (input size, number of examples)\n    parameters -- output of initialize_parameters_deep()\n    \n    Returns:\n    AL -- activation value from the output (last) layer\n    caches -- list of caches containing:\n                every cache of linear_activation_forward() (there are L of them, indexed from 0 to L-1)\n    \"\"\"\n    \n    # find number of layers\n    L = len(parameters)//2\n    \n    # get ready to capture all caches from each layer\n    caches = []\n    \n    # initialise A_prev as X\n    A_prev = X\n    \n    # forward propagation for layers (aside from last)\n    for l in range(1, L):\n        \n        # compute forward linear activation\n        A, cache = linear_activation_forward(A_prev, parameters['W'+str(l)], parameters['b'+str(l)], activation='relu')\n\n        # reset A_prev\n        A_prev = A\n        \n        caches.append(cache)\n    \n    # forward propagation for last layer\n    AL, cache = linear_activation_forward(A_prev, parameters['W'+str(L)], parameters['b'+str(L)], activation='softmax')\n    caches.append(cache)\n    \n    return AL, caches","metadata":{"_uuid":"d9fd0623-a911-4467-a790-8171d8794c66","_cell_guid":"fbf03092-5b56-4a66-a7c4-f36e313fb643","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-27T19:19:34.128283Z","iopub.execute_input":"2024-06-27T19:19:34.128856Z","iopub.status.idle":"2024-06-27T19:19:34.146014Z","shell.execute_reply.started":"2024-06-27T19:19:34.128812Z","shell.execute_reply":"2024-06-27T19:19:34.144704Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"AL, caches = l_model_forward(X_train[:,:5], parameters)\n\n# final scores for first 5 training examples\nAL","metadata":{"_uuid":"79b60d27-2814-4b3b-9d17-5078ccc19a3d","_cell_guid":"9465b078-62c5-4e38-b5f4-9343b4b7e4f0","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-27T20:34:49.122495Z","iopub.execute_input":"2024-06-27T20:34:49.12293Z","iopub.status.idle":"2024-06-27T20:34:49.130653Z","shell.execute_reply.started":"2024-06-27T20:34:49.122897Z","shell.execute_reply":"2024-06-27T20:34:49.129632Z"},"trusted":true},"execution_count":72,"outputs":[{"execution_count":72,"output_type":"execute_result","data":{"text/plain":"array([[0.10043734, 0.09816342, 0.09985779, 0.100368  , 0.09830082],\n       [0.10047614, 0.09973125, 0.10069528, 0.10190497, 0.09858928],\n       [0.09982359, 0.09996954, 0.1016143 , 0.10114871, 0.10389063],\n       [0.10018377, 0.09900047, 0.09954793, 0.1005564 , 0.0984937 ],\n       [0.09845455, 0.10091609, 0.0964003 , 0.09509867, 0.09903717],\n       [0.10081104, 0.1009915 , 0.10181348, 0.10313938, 0.10124601],\n       [0.1000292 , 0.09894323, 0.09707522, 0.09707375, 0.09379193],\n       [0.10080719, 0.1028208 , 0.1046786 , 0.10427823, 0.10618434],\n       [0.09988401, 0.10053903, 0.10085571, 0.09934869, 0.10202048],\n       [0.09909318, 0.09892468, 0.09746139, 0.0970832 , 0.09844565]])"},"metadata":{}}]},{"cell_type":"markdown","source":"# 4. Compute cost","metadata":{}},{"cell_type":"code","source":"def onehot_encoder(array):\n    \n    ohe = np.zeros((len(array), array.max()+1))\n    ohe[np.arange(len(array)), array] = 1\n    ohe = ohe.transpose()\n    \n    return ohe","metadata":{"_uuid":"6b425b0a-51a6-47d9-942b-99d683503840","_cell_guid":"0c63d23b-669b-4801-8d6a-58037f490f24","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-27T20:34:50.614447Z","iopub.execute_input":"2024-06-27T20:34:50.615134Z","iopub.status.idle":"2024-06-27T20:34:50.619888Z","shell.execute_reply.started":"2024-06-27T20:34:50.615105Z","shell.execute_reply":"2024-06-27T20:34:50.618945Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"def compute_cost(AL, y):\n    \n    # one hot encode y\n    y_ohe = onehot_encoder(y)\n    \n    # number of examples\n    m = len(y)\n    \n    # categorical cross-entropy loss function\n    cost = (-1/m) * np.sum(y_ohe * np.log(AL))\n    \n    return cost","metadata":{"_uuid":"309754f0-881b-4370-943d-a65a13f77485","_cell_guid":"fc3e414e-bfb1-4590-a4f7-99255e46b3c4","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-27T20:34:51.642401Z","iopub.execute_input":"2024-06-27T20:34:51.643191Z","iopub.status.idle":"2024-06-27T20:34:51.647768Z","shell.execute_reply.started":"2024-06-27T20:34:51.643159Z","shell.execute_reply":"2024-06-27T20:34:51.646774Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"compute_cost(AL, y_train[:5])","metadata":{"_uuid":"a4de3529-3b0b-4c4b-83e6-b6ccc1bf8bba","_cell_guid":"1812963f-5cd9-4388-92c5-ea77a9d95000","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-27T20:35:03.344954Z","iopub.execute_input":"2024-06-27T20:35:03.345326Z","iopub.status.idle":"2024-06-27T20:35:03.351915Z","shell.execute_reply.started":"2024-06-27T20:35:03.345296Z","shell.execute_reply":"2024-06-27T20:35:03.350963Z"},"trusted":true},"execution_count":76,"outputs":[{"execution_count":76,"output_type":"execute_result","data":{"text/plain":"2.3104179179081976"},"metadata":{}}]},{"cell_type":"code","source":"def get_predictions(AL):\n    predictions = np.argmax(AL, axis=0)\n    return predictions","metadata":{"execution":{"iopub.status.busy":"2024-06-27T20:35:05.594293Z","iopub.execute_input":"2024-06-27T20:35:05.595106Z","iopub.status.idle":"2024-06-27T20:35:05.599393Z","shell.execute_reply.started":"2024-06-27T20:35:05.595075Z","shell.execute_reply":"2024-06-27T20:35:05.598356Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"predictions = get_predictions(AL)\npredictions","metadata":{"execution":{"iopub.status.busy":"2024-06-27T20:35:10.174374Z","iopub.execute_input":"2024-06-27T20:35:10.175001Z","iopub.status.idle":"2024-06-27T20:35:10.181482Z","shell.execute_reply.started":"2024-06-27T20:35:10.174966Z","shell.execute_reply":"2024-06-27T20:35:10.180551Z"},"trusted":true},"execution_count":78,"outputs":[{"execution_count":78,"output_type":"execute_result","data":{"text/plain":"array([5, 7, 7, 7, 7])"},"metadata":{}}]},{"cell_type":"code","source":"def compute_accuracy(predictions, Y):\n    \n    accuracy = np.sum(predictions==Y)/Y.size\n    \n    return accuracy","metadata":{"execution":{"iopub.status.busy":"2024-06-27T20:35:11.958413Z","iopub.execute_input":"2024-06-27T20:35:11.958787Z","iopub.status.idle":"2024-06-27T20:35:11.963416Z","shell.execute_reply.started":"2024-06-27T20:35:11.958757Z","shell.execute_reply":"2024-06-27T20:35:11.962453Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"markdown","source":"# 5. Backward propagation","metadata":{}},{"cell_type":"code","source":"def linear_backward(dZ, cache):\n    \n    A_prev, W, b = cache\n    m = A_prev.shape[1]\n    \n    dA_prev = np.dot(W.T, dZ)\n    dW = (1/m)*np.dot(dZ, A_prev.T)\n    db = (1/m)*np.sum(dZ, axis=1, keepdims=True)\n    \n    return dA_prev, dW, db","metadata":{"_uuid":"79bf37d2-2461-4f12-bd37-d3c4a128c0aa","_cell_guid":"7f244b2c-da8c-4be4-b214-df61f4d446b5","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-27T19:19:34.273943Z","iopub.execute_input":"2024-06-27T19:19:34.274483Z","iopub.status.idle":"2024-06-27T19:19:34.285411Z","shell.execute_reply.started":"2024-06-27T19:19:34.274439Z","shell.execute_reply":"2024-06-27T19:19:34.284297Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def linear_activation_backward(dA, cache, Y, activation_function):\n    \n    linear_cache, activation_cache = cache\n    Z = activation_cache\n    \n    # calculation depends on activation function\n    if activation_function=='sigmoid':\n        \n        dZ = dA * (sigmoid(Z)*(1-sigmoid(Z)))\n        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n        \n    elif activation_function=='softmax':\n        \n        dZ = softmax(Z) - Y\n        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n        \n    elif activation_function=='relu':\n        \n        dZ = np.array(dA, copy=True)\n        dZ[Z <= 0] = 0\n        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n        \n    return dA_prev, dW, db","metadata":{"_uuid":"87e3b7e1-4f34-407b-b1c4-34f3a9063c70","_cell_guid":"fbf2aef5-540f-424a-bfea-6d98b368478d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-27T19:19:34.286873Z","iopub.execute_input":"2024-06-27T19:19:34.287862Z","iopub.status.idle":"2024-06-27T19:19:34.299208Z","shell.execute_reply.started":"2024-06-27T19:19:34.287821Z","shell.execute_reply":"2024-06-27T19:19:34.298393Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def l_model_backward(AL, Y, caches):\n    \"\"\"\n    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n    \n    Arguments:\n    AL -- probability vector, output of the forward propagation (L_model_forward())\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n    caches -- list of caches containing:\n                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n    \n    Returns:\n    grads -- A dictionary with the gradients\n             grads[\"dA\" + str(l)] = ... \n             grads[\"dW\" + str(l)] = ... \n             grads[\"db\" + str(l)] = ... \n    \"\"\"\n    \n    # number of layers\n    L = len(caches)\n    \n    # gradient storage\n    grads = dict()    \n    \n    # one hot encode y\n    Y_ohe = onehot_encoder(Y)\n    \n    # initiate backward propagation\n#     print('Y:',Y_ohe)\n#     print('AL:', AL)\n    dAL = -np.divide(Y_ohe, AL) + np.divide(1-Y_ohe, 1-AL)\n    \n    # propagate backwards through last layer separately (as it has a different activation function)\n    current_cache = caches[L-1]\n#     print('layer:', L)\n    grads['dA'+str(L-1)], grads['dW'+str(L)], grads['db'+str(L)] = linear_activation_backward(dAL, current_cache, Y_ohe, activation_function='softmax')\n    \n    # cycle through layers in reverse\n    for l in reversed(range(1, L)):\n        current_cache = caches[l-1]\n#         print('layer:', l)\n        grads['dA'+str(l-1)], grads['dW'+str(l)], grads['db'+str(l)] = linear_activation_backward(grads['dA'+str(l)], current_cache, Y_ohe, activation_function='relu')\n        \n    return grads","metadata":{"_uuid":"01fd2c32-4ac0-4f9f-a1c3-a8eeff5bff9d","_cell_guid":"95459881-d7e8-4b98-ab01-14de00c8eb3e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-27T19:19:34.300422Z","iopub.execute_input":"2024-06-27T19:19:34.300925Z","iopub.status.idle":"2024-06-27T19:19:34.311573Z","shell.execute_reply.started":"2024-06-27T19:19:34.300897Z","shell.execute_reply":"2024-06-27T19:19:34.310871Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"# 6. Update parameters","metadata":{}},{"cell_type":"code","source":"def update_parameters(params, grads, learning_rate):\n    \n    L = len(params)//2\n        \n    # cycle through each layer and change parameters\n    for l in range(1, L+1):\n        params['W'+str(l)] = params['W'+str(l)] - learning_rate * grads['dW'+str(l)]\n        params['b'+str(l)] = params['b'+str(l)] - learning_rate * grads['db'+str(l)]\n        \n    return params","metadata":{"_uuid":"a18fe695-1bfd-4d47-be03-7b1b597570f0","_cell_guid":"40c97b9d-7983-4bba-9db5-416cf4a9984d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-27T19:19:34.312725Z","iopub.execute_input":"2024-06-27T19:19:34.312952Z","iopub.status.idle":"2024-06-27T19:19:34.324301Z","shell.execute_reply.started":"2024-06-27T19:19:34.312932Z","shell.execute_reply":"2024-06-27T19:19:34.323427Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"# 7. Putting it all together","metadata":{}},{"cell_type":"code","source":"def l_layer_model(X, Y, layers_dim, learning_rate=0.001, num_iterations=100, print_cost=True):\n    \"\"\"\n    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n    \n    Arguments:\n    X -- input data, of shape (n_x, number of examples)\n    Y -- true \"label\" vector (containing 1 if cat, 0 if non-cat), of shape (1, number of examples)\n    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n    learning_rate -- learning rate of the gradient descent update rule\n    num_iterations -- number of iterations of the optimization loop\n    print_cost -- if True, it prints the cost every 100 steps\n    \n    Returns:\n    parameters -- parameters learnt by the model. They can then be used to predict.\n    \"\"\"\n    \n    # initialise parameters\n    params = initialise_parameters(layers_dim)\n#     print('Initial W3:', params['W3'], params['W3'].shape)\n\n    # repeat for number of iterations\n    for i in range(0, num_iterations):\n\n        # forward propagation\n        AL, caches = l_model_forward(X, params)\n\n        # compute cost - TRY REPLACING WITH PYTORTH EQUIVALENT\n        cost = compute_cost(AL, Y)\n\n        # backward propagation\n        grads = l_model_backward(AL, Y, caches)\n#         print(grads)\n        \n        # update parameters\n        params = update_parameters(params, grads, learning_rate)\n#         print(params)\n                \n        # get predictions\n        predictions = get_predictions(AL)\n        accuracy = compute_accuracy(predictions, Y)\n        # print cost if required\n        if print_cost==True and i%10==0:\n            print('Iteration',i,'- cost:', cost)\n            print('Accuracy:', accuracy)\n    \n    return params","metadata":{"_uuid":"03a5d750-7c50-420a-a606-3efd17e5f2e1","_cell_guid":"3a9ef6f1-8624-41ab-8495-62728a6b775c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-27T19:19:34.325422Z","iopub.execute_input":"2024-06-27T19:19:34.325692Z","iopub.status.idle":"2024-06-27T19:19:34.338306Z","shell.execute_reply.started":"2024-06-27T19:19:34.32567Z","shell.execute_reply":"2024-06-27T19:19:34.337526Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"model_params = l_layer_model(X_train, y_train, [784, 15, 10])","metadata":{"_uuid":"6c8010b6-f81d-4b28-bcea-ce543ba66e5e","_cell_guid":"656ea5bd-9909-4717-90ca-56b2d139eda7","execution":{"iopub.status.busy":"2024-06-27T19:19:34.339281Z","iopub.execute_input":"2024-06-27T19:19:34.339527Z","iopub.status.idle":"2024-06-27T19:20:25.229891Z","shell.execute_reply.started":"2024-06-27T19:19:34.339505Z","shell.execute_reply":"2024-06-27T19:20:25.228558Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Iteration 0 - cost: 2.563614835226048\nAccuracy: 0.11902380952380952\nIteration 10 - cost: 1.716749477720245\nAccuracy: 0.4903571428571429\nIteration 20 - cost: 1.1856021685176998\nAccuracy: 0.6793333333333333\nIteration 30 - cost: 0.8610170403972757\nAccuracy: 0.7602619047619048\nIteration 40 - cost: 0.7046903144654633\nAccuracy: 0.7941428571428572\nIteration 50 - cost: 0.620289631872739\nAccuracy: 0.8155\nIteration 60 - cost: 0.5666639255037882\nAccuracy: 0.8302380952380952\nIteration 70 - cost: 0.5290612121371051\nAccuracy: 0.8415238095238096\nIteration 80 - cost: 0.5009864108261992\nAccuracy: 0.8507380952380953\nIteration 90 - cost: 0.4790268429458978\nAccuracy: 0.8579761904761904\n","output_type":"stream"}]},{"cell_type":"code","source":"def predict(X, parameters):\n    \n    # get scores after forward prop\n    AL, _ = l_model_forward(X, parameters)\n    \n    # get predictions\n    predictions = get_predictions(AL)\n    \n    return predictions\n\ndef test_prediction(index, parameters):\n    \n    # get prediction for example\n    prediction = predict(X_train[:, [index]], parameters)\n    \n    # get label\n    label = y_train[index]\n    \n    # show image\n    plt.imshow(X_train[:, [index]].reshape(28, 28))\n    print('Label:', label)\n    print('Prediction:', prediction)\n    ","metadata":{"_uuid":"b26cd072-6ae9-4f23-8296-ec1485ea42f3","_cell_guid":"fbc7c220-4736-4de5-9d7f-794fefb411a3","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-27T19:20:25.231441Z","iopub.execute_input":"2024-06-27T19:20:25.231963Z","iopub.status.idle":"2024-06-27T19:20:25.239786Z","shell.execute_reply.started":"2024-06-27T19:20:25.231931Z","shell.execute_reply":"2024-06-27T19:20:25.238422Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"predict(X_train[:, [100]], model_params)","metadata":{"_uuid":"848e52ef-a83c-4848-8a2b-44a187f9ddc6","_cell_guid":"4bb5359c-e8ee-4e16-bbaa-89243a5097f8","execution":{"iopub.status.busy":"2024-06-27T19:20:25.241507Z","iopub.execute_input":"2024-06-27T19:20:25.242183Z","iopub.status.idle":"2024-06-27T19:20:25.257838Z","shell.execute_reply.started":"2024-06-27T19:20:25.242152Z","shell.execute_reply":"2024-06-27T19:20:25.256436Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"array([9])"},"metadata":{}}]},{"cell_type":"code","source":"test_prediction(590, model_params)","metadata":{"execution":{"iopub.status.busy":"2024-06-27T19:20:25.267402Z","iopub.execute_input":"2024-06-27T19:20:25.268218Z","iopub.status.idle":"2024-06-27T19:20:25.527494Z","shell.execute_reply.started":"2024-06-27T19:20:25.268176Z","shell.execute_reply":"2024-06-27T19:20:25.526638Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Label: 1\nPrediction: [1]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"markdown","source":"# Hyperparameter tuning","metadata":{}},{"cell_type":"code","source":"np.random.seed(42)\n\n# shuffle dataset\nindices = np.arange(train.shape[0])\nnp.random.shuffle(indices)\ntrain_shuffled = train[indices]\n\n# validation ratio\nvalidation_ratio = 0.2\n\n# split index\nsplit_index = int(train_shuffled.shape[0]*(1-validation_ratio))\n\n# split data\ntrain_split = train_shuffled[:split_index]\nvalidation_split = train_shuffled[split_index:]\n\n# split features and target variable\nX_train = train_split[:,1:].transpose()\ny_train = train_split[:,0].transpose()\nX_val = validation_split[:,1:].transpose()\ny_val = validation_split[:,0].transpose()","metadata":{"execution":{"iopub.status.busy":"2024-06-27T19:20:25.749897Z","iopub.execute_input":"2024-06-27T19:20:25.750247Z","iopub.status.idle":"2024-06-27T19:20:26.13983Z","shell.execute_reply.started":"2024-06-27T19:20:25.750207Z","shell.execute_reply":"2024-06-27T19:20:26.139076Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# hyperparameter space\nparameter_grid = {\n    'layer_dims':[\n                [784, 15, 10], # small nn\n                [784, 20, 15, 10], # medium nn\n                [784, 50, 25, 15, 10] # large nn\n    ], \n    'learning_rate':[0.01, 0.001],\n    'num_iterations':[250, 100]\n}","metadata":{"execution":{"iopub.status.busy":"2024-06-27T19:32:35.72554Z","iopub.execute_input":"2024-06-27T19:32:35.726373Z","iopub.status.idle":"2024-06-27T19:32:35.731086Z","shell.execute_reply.started":"2024-06-27T19:32:35.726341Z","shell.execute_reply":"2024-06-27T19:32:35.730178Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"import itertools\n\nkeys, values = zip(*parameter_grid.items())\nresults = []\nfor parameter_combo in itertools.product(*values):\n    \n    # create parameter combination dictionary\n    params = dict(zip(keys, parameter_combo))\n    print(params)\n    \n    # train model\n    model_params = l_layer_model(X_train, \n                                 y_train,\n                                 layers_dim=params['layer_dims'],\n                                 learning_rate=params['learning_rate'],\n                                 num_iterations=params['num_iterations'],\n                                 print_cost=True)\n    \n    \n    # make predictions on the test set\n    preds = predict(X_val, model_params)\n    \n    # get accuracy\n    accuracy = compute_accuracy(preds, y_val)\n    print(accuracy)\n    \n    # save results\n    result = [params, model_params, accuracy]\n    results.append(result)","metadata":{"execution":{"iopub.status.busy":"2024-06-27T19:34:20.219578Z","iopub.execute_input":"2024-06-27T19:34:20.219945Z","iopub.status.idle":"2024-06-27T19:50:32.937831Z","shell.execute_reply.started":"2024-06-27T19:34:20.219917Z","shell.execute_reply":"2024-06-27T19:50:32.936574Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"{'layer_dims': [784, 15, 10], 'learning_rate': 0.01, 'num_iterations': 250}\nIteration 0 - cost: 2.4863953225185442\nAccuracy: 0.07172619047619047\nIteration 10 - cost: 2.4585263029278552\nAccuracy: 0.2505654761904762\nIteration 20 - cost: 2.122554174685665\nAccuracy: 0.17095238095238094\nIteration 30 - cost: 2.2996461460245725\nAccuracy: 0.11726190476190476\nIteration 40 - cost: 2.0724583430920807\nAccuracy: 0.2051488095238095\nIteration 50 - cost: 2.0917747390391743\nAccuracy: 0.11202380952380953\nIteration 60 - cost: 2.074103843704822\nAccuracy: 0.2037797619047619\nIteration 70 - cost: 2.0684985594649854\nAccuracy: 0.2044642857142857\nIteration 80 - cost: 2.065403477512458\nAccuracy: 0.2049404761904762\nIteration 90 - cost: 2.0631370688536927\nAccuracy: 0.20535714285714285\nIteration 100 - cost: 2.0612199740253545\nAccuracy: 0.20541666666666666\nIteration 110 - cost: 2.059559653184115\nAccuracy: 0.2055357142857143\nIteration 120 - cost: 2.0580894776240997\nAccuracy: 0.21220238095238095\nIteration 130 - cost: 2.056769759943511\nAccuracy: 0.21217261904761905\nIteration 140 - cost: 2.0555575903796597\nAccuracy: 0.21226190476190476\nIteration 150 - cost: 2.0544062019685385\nAccuracy: 0.21241071428571429\nIteration 160 - cost: 2.053311051079659\nAccuracy: 0.2124702380952381\nIteration 170 - cost: 2.0522601963562845\nAccuracy: 0.2125892857142857\nIteration 180 - cost: 2.051230606864867\nAccuracy: 0.21279761904761904\nIteration 190 - cost: 2.050232579740663\nAccuracy: 0.2125595238095238\nIteration 200 - cost: 2.0492709075361826\nAccuracy: 0.21252976190476192\nIteration 210 - cost: 2.0483437539386022\nAccuracy: 0.21252976190476192\nIteration 220 - cost: 2.0474416804926223\nAccuracy: 0.2125892857142857\nIteration 230 - cost: 2.0465552435523175\nAccuracy: 0.21282738095238096\nIteration 240 - cost: 2.0456860108931845\nAccuracy: 0.21291666666666667\n0.20904761904761904\n{'layer_dims': [784, 15, 10], 'learning_rate': 0.01, 'num_iterations': 100}\nIteration 0 - cost: 2.52363470609526\nAccuracy: 0.09422619047619048\nIteration 10 - cost: 2.2819842745349845\nAccuracy: 0.1286904761904762\nIteration 20 - cost: 1.9455295775335915\nAccuracy: 0.29696428571428574\nIteration 30 - cost: 1.8365175622792447\nAccuracy: 0.34285714285714286\nIteration 40 - cost: 1.9172131508833195\nAccuracy: 0.2974702380952381\nIteration 50 - cost: 1.4115830219749161\nAccuracy: 0.49949404761904764\nIteration 60 - cost: 1.7618243167464196\nAccuracy: 0.35910714285714285\nIteration 70 - cost: 1.6755506088103351\nAccuracy: 0.38529761904761906\nIteration 80 - cost: 1.6645197362270707\nAccuracy: 0.38666666666666666\nIteration 90 - cost: 1.6529367573875575\nAccuracy: 0.3857738095238095\n0.37583333333333335\n{'layer_dims': [784, 15, 10], 'learning_rate': 0.001, 'num_iterations': 250}\nIteration 0 - cost: 2.381118344945098\nAccuracy: 0.08720238095238095\nIteration 10 - cost: 1.752539233484761\nAccuracy: 0.4463690476190476\nIteration 20 - cost: 1.2160680166636677\nAccuracy: 0.6618452380952381\nIteration 30 - cost: 0.9005711776145805\nAccuracy: 0.734375\nIteration 40 - cost: 0.7420722870650025\nAccuracy: 0.7774702380952381\nIteration 50 - cost: 0.6506004908348855\nAccuracy: 0.8034821428571428\nIteration 60 - cost: 0.5913414115961432\nAccuracy: 0.82125\nIteration 70 - cost: 0.5495107547604552\nAccuracy: 0.8338095238095238\nIteration 80 - cost: 0.5180297242018506\nAccuracy: 0.8441071428571428\nIteration 90 - cost: 0.49327312453909455\nAccuracy: 0.851875\nIteration 100 - cost: 0.47315013550232776\nAccuracy: 0.8583035714285714\nIteration 110 - cost: 0.456397107192849\nAccuracy: 0.8629166666666667\nIteration 120 - cost: 0.44219374989419497\nAccuracy: 0.8679761904761905\nIteration 130 - cost: 0.42993206742196716\nAccuracy: 0.8718154761904762\nIteration 140 - cost: 0.4191992339385772\nAccuracy: 0.8750595238095238\nIteration 150 - cost: 0.40970684233152127\nAccuracy: 0.878125\nIteration 160 - cost: 0.40122791239652444\nAccuracy: 0.880625\nIteration 170 - cost: 0.39357787521855947\nAccuracy: 0.8833333333333333\nIteration 180 - cost: 0.38662365117394976\nAccuracy: 0.8854464285714285\nIteration 190 - cost: 0.38024946263556786\nAccuracy: 0.8876488095238095\nIteration 200 - cost: 0.37438590405309036\nAccuracy: 0.8896428571428572\nIteration 210 - cost: 0.36896804381387915\nAccuracy: 0.8908928571428572\nIteration 220 - cost: 0.36392785054894294\nAccuracy: 0.892172619047619\nIteration 230 - cost: 0.3592103562717576\nAccuracy: 0.8934821428571429\nIteration 240 - cost: 0.35478029273395834\nAccuracy: 0.8945833333333333\n0.893452380952381\n{'layer_dims': [784, 15, 10], 'learning_rate': 0.001, 'num_iterations': 100}\nIteration 0 - cost: 2.3881028010113092\nAccuracy: 0.07955357142857143\nIteration 10 - cost: 1.8416301999679603\nAccuracy: 0.38470238095238096\nIteration 20 - cost: 1.3448925679925456\nAccuracy: 0.595297619047619\nIteration 30 - cost: 0.9944309824092826\nAccuracy: 0.6949702380952381\nIteration 40 - cost: 0.8103282447812356\nAccuracy: 0.7551488095238095\nIteration 50 - cost: 0.7020716530043144\nAccuracy: 0.7918452380952381\nIteration 60 - cost: 0.6294418836422612\nAccuracy: 0.8141071428571428\nIteration 70 - cost: 0.5767262464491124\nAccuracy: 0.830297619047619\nIteration 80 - cost: 0.5368225905082912\nAccuracy: 0.8413392857142857\nIteration 90 - cost: 0.5057816849909016\nAccuracy: 0.8500297619047619\n0.861547619047619\n{'layer_dims': [784, 20, 15, 10], 'learning_rate': 0.01, 'num_iterations': 250}\nIteration 0 - cost: 2.305241049577332\nAccuracy: 0.0880952380952381\nIteration 10 - cost: 2.2852185413930663\nAccuracy: 0.15946428571428573\nIteration 20 - cost: 2.204990241396742\nAccuracy: 0.12122023809523809\nIteration 30 - cost: 1.980901502107701\nAccuracy: 0.3219940476190476\nIteration 40 - cost: 1.3044980098191348\nAccuracy: 0.6005357142857143\nIteration 50 - cost: 1.3332745968167725\nAccuracy: 0.5610416666666667\nIteration 60 - cost: 1.109051500712785\nAccuracy: 0.5885416666666666\nIteration 70 - cost: 0.9911130704297875\nAccuracy: 0.6348214285714285\nIteration 80 - cost: 1.1699663889053868\nAccuracy: 0.5589583333333333\nIteration 90 - cost: 0.8413740386583145\nAccuracy: 0.6757738095238095\nIteration 100 - cost: 0.6175097296910652\nAccuracy: 0.8011607142857143\nIteration 110 - cost: 0.5582796070976985\nAccuracy: 0.8238690476190477\nIteration 120 - cost: 0.609563810585016\nAccuracy: 0.7855059523809523\nIteration 130 - cost: 0.47191858893352717\nAccuracy: 0.8523214285714286\nIteration 140 - cost: 0.5139858470659177\nAccuracy: 0.8261309523809524\nIteration 150 - cost: 0.4202570389128631\nAccuracy: 0.8720238095238095\nIteration 160 - cost: 0.44597652739526944\nAccuracy: 0.8671428571428571\nIteration 170 - cost: 0.3789695997814244\nAccuracy: 0.8886011904761905\nIteration 180 - cost: 0.35563706156896135\nAccuracy: 0.8944345238095238\nIteration 190 - cost: 0.34928189912048846\nAccuracy: 0.8957142857142857\nIteration 200 - cost: 0.34601827634176296\nAccuracy: 0.8972916666666667\nIteration 210 - cost: 0.3227512253246025\nAccuracy: 0.9040178571428571\nIteration 220 - cost: 0.30966930322605174\nAccuracy: 0.9070833333333334\nIteration 230 - cost: 0.3083418393746348\nAccuracy: 0.9086309523809524\nIteration 240 - cost: 0.3222582909224661\nAccuracy: 0.9045833333333333\n0.9158333333333334\n{'layer_dims': [784, 20, 15, 10], 'learning_rate': 0.01, 'num_iterations': 100}\nIteration 0 - cost: 2.2989923756550907\nAccuracy: 0.12970238095238096\nIteration 10 - cost: 2.28090930609932\nAccuracy: 0.15979166666666667\nIteration 20 - cost: 2.1824943190889377\nAccuracy: 0.18044642857142856\nIteration 30 - cost: 1.854968882416849\nAccuracy: 0.2836309523809524\nIteration 40 - cost: 2.006154768844876\nAccuracy: 0.21419642857142857\nIteration 50 - cost: 1.6650295284370351\nAccuracy: 0.36163690476190474\nIteration 60 - cost: 1.2139298508547332\nAccuracy: 0.5610416666666667\nIteration 70 - cost: 2.5354142154580304\nAccuracy: 0.3175595238095238\nIteration 80 - cost: 1.0560822214944965\nAccuracy: 0.6138690476190476\nIteration 90 - cost: 1.952811874319036\nAccuracy: 0.415327380952381\n0.7627380952380952\n{'layer_dims': [784, 20, 15, 10], 'learning_rate': 0.001, 'num_iterations': 250}\nIteration 0 - cost: 2.300755581629324\nAccuracy: 0.08598214285714285\nIteration 10 - cost: 2.2999599224772576\nAccuracy: 0.11235119047619048\nIteration 20 - cost: 2.299133304782242\nAccuracy: 0.13729166666666667\nIteration 30 - cost: 2.298259407934324\nAccuracy: 0.1543452380952381\nIteration 40 - cost: 2.297321113690952\nAccuracy: 0.16586309523809523\nIteration 50 - cost: 2.29629799042237\nAccuracy: 0.17511904761904762\nIteration 60 - cost: 2.2951709078550953\nAccuracy: 0.18172619047619049\nIteration 70 - cost: 2.2939183064750246\nAccuracy: 0.18773809523809523\nIteration 80 - cost: 2.2925178543581235\nAccuracy: 0.19279761904761905\nIteration 90 - cost: 2.2909400101190713\nAccuracy: 0.19699404761904762\nIteration 100 - cost: 2.289152860677205\nAccuracy: 0.20080357142857144\nIteration 110 - cost: 2.2871215262410547\nAccuracy: 0.2042857142857143\nIteration 120 - cost: 2.284803970200838\nAccuracy: 0.20735119047619047\nIteration 130 - cost: 2.282148507640838\nAccuracy: 0.21035714285714285\nIteration 140 - cost: 2.2790809157498937\nAccuracy: 0.213125\nIteration 150 - cost: 2.2755233458301807\nAccuracy: 0.21595238095238095\nIteration 160 - cost: 2.271372812815109\nAccuracy: 0.21848214285714285\nIteration 170 - cost: 2.2664967479709834\nAccuracy: 0.2205952380952381\nIteration 180 - cost: 2.260730587084773\nAccuracy: 0.22273809523809524\nIteration 190 - cost: 2.253879164875418\nAccuracy: 0.22517857142857142\nIteration 200 - cost: 2.245689475477621\nAccuracy: 0.22770833333333335\nIteration 210 - cost: 2.2358346713538313\nAccuracy: 0.23086309523809523\nIteration 220 - cost: 2.223917632271498\nAccuracy: 0.2338095238095238\nIteration 230 - cost: 2.2094326014611445\nAccuracy: 0.23705357142857142\nIteration 240 - cost: 2.1917741179411085\nAccuracy: 0.2413095238095238\n0.2386904761904762\n{'layer_dims': [784, 20, 15, 10], 'learning_rate': 0.001, 'num_iterations': 100}\nIteration 0 - cost: 2.3035982820003373\nAccuracy: 0.11258928571428571\nIteration 10 - cost: 2.3027762739730555\nAccuracy: 0.11410714285714285\nIteration 20 - cost: 2.3019832603723063\nAccuracy: 0.11580357142857142\nIteration 30 - cost: 2.3012020772996293\nAccuracy: 0.11708333333333333\nIteration 40 - cost: 2.3004151230003944\nAccuracy: 0.11892857142857143\nIteration 50 - cost: 2.2996006149936012\nAccuracy: 0.12074404761904763\nIteration 60 - cost: 2.298737017740819\nAccuracy: 0.12300595238095238\nIteration 70 - cost: 2.2978048012507992\nAccuracy: 0.12526785714285715\nIteration 80 - cost: 2.296779051693235\nAccuracy: 0.12755952380952382\nIteration 90 - cost: 2.2956275675810516\nAccuracy: 0.13160714285714287\n0.12857142857142856\n{'layer_dims': [784, 50, 25, 15, 10], 'learning_rate': 0.01, 'num_iterations': 250}\nIteration 0 - cost: 2.302608586059759\nAccuracy: 0.0911904761904762\nIteration 10 - cost: 2.3025617586517195\nAccuracy: 0.11535714285714285\nIteration 20 - cost: 2.3025151805193813\nAccuracy: 0.11229166666666666\nIteration 30 - cost: 2.3024686316058123\nAccuracy: 0.11202380952380953\nIteration 40 - cost: 2.3024218978632454\nAccuracy: 0.11202380952380953\nIteration 50 - cost: 2.3023747567112887\nAccuracy: 0.11202380952380953\nIteration 60 - cost: 2.302326969200421\nAccuracy: 0.11202380952380953\nIteration 70 - cost: 2.3022782742456065\nAccuracy: 0.11202380952380953\nIteration 80 - cost: 2.3022284547051544\nAccuracy: 0.11202380952380953\nIteration 90 - cost: 2.3021771922845153\nAccuracy: 0.11202380952380953\nIteration 100 - cost: 2.3021242625226406\nAccuracy: 0.11202380952380953\nIteration 110 - cost: 2.3020693977794133\nAccuracy: 0.11202380952380953\nIteration 120 - cost: 2.302012302671986\nAccuracy: 0.11202380952380953\nIteration 130 - cost: 2.301952638050507\nAccuracy: 0.11202380952380953\nIteration 140 - cost: 2.30189017582273\nAccuracy: 0.11202380952380953\nIteration 150 - cost: 2.3018242459400367\nAccuracy: 0.11202380952380953\nIteration 160 - cost: 2.3017541196381672\nAccuracy: 0.11202380952380953\nIteration 170 - cost: 2.301679232902304\nAccuracy: 0.11202380952380953\nIteration 180 - cost: 2.3015986045838055\nAccuracy: 0.11202380952380953\nIteration 190 - cost: 2.301511222984478\nAccuracy: 0.11202380952380953\nIteration 200 - cost: 2.301415883071746\nAccuracy: 0.11202380952380953\nIteration 210 - cost: 2.3013109747403733\nAccuracy: 0.11202380952380953\nIteration 220 - cost: 2.3011943718795465\nAccuracy: 0.11202380952380953\nIteration 230 - cost: 2.3010636745316235\nAccuracy: 0.11202380952380953\nIteration 240 - cost: 2.300915784902462\nAccuracy: 0.11202380952380953\n0.10952380952380952\n{'layer_dims': [784, 50, 25, 15, 10], 'learning_rate': 0.01, 'num_iterations': 100}\nIteration 0 - cost: 2.3025109624484044\nAccuracy: 0.10505952380952381\nIteration 10 - cost: 2.302445327724762\nAccuracy: 0.1749702380952381\nIteration 20 - cost: 2.3023814419756947\nAccuracy: 0.13833333333333334\nIteration 30 - cost: 2.3023188687245706\nAccuracy: 0.11571428571428571\nIteration 40 - cost: 2.302256921256991\nAccuracy: 0.11244047619047619\nIteration 50 - cost: 2.3021948560112837\nAccuracy: 0.11205357142857143\nIteration 60 - cost: 2.302132040599186\nAccuracy: 0.11202380952380953\nIteration 70 - cost: 2.302067959097319\nAccuracy: 0.11202380952380953\nIteration 80 - cost: 2.3020021041911307\nAccuracy: 0.11202380952380953\nIteration 90 - cost: 2.301933792578905\nAccuracy: 0.11202380952380953\n0.10952380952380952\n{'layer_dims': [784, 50, 25, 15, 10], 'learning_rate': 0.001, 'num_iterations': 250}\nIteration 0 - cost: 2.302633761366576\nAccuracy: 0.07630952380952381\nIteration 10 - cost: 2.3026281251710015\nAccuracy: 0.07395833333333333\nIteration 20 - cost: 2.3026224926304524\nAccuracy: 0.07511904761904761\nIteration 30 - cost: 2.3026168667503706\nAccuracy: 0.08038690476190476\nIteration 40 - cost: 2.3026112481418632\nAccuracy: 0.08654761904761905\nIteration 50 - cost: 2.302605638767212\nAccuracy: 0.09241071428571429\nIteration 60 - cost: 2.3026000379343263\nAccuracy: 0.09738095238095239\nIteration 70 - cost: 2.3025944446218634\nAccuracy: 0.10050595238095238\nIteration 80 - cost: 2.3025888560284757\nAccuracy: 0.10110119047619047\nIteration 90 - cost: 2.302583271014026\nAccuracy: 0.10229166666666667\nIteration 100 - cost: 2.302577690193971\nAccuracy: 0.1030952380952381\nIteration 110 - cost: 2.3025721119607208\nAccuracy: 0.10413690476190476\nIteration 120 - cost: 2.3025665353884905\nAccuracy: 0.10497023809523809\nIteration 130 - cost: 2.3025609628044603\nAccuracy: 0.1055654761904762\nIteration 140 - cost: 2.3025553957025036\nAccuracy: 0.10622023809523809\nIteration 150 - cost: 2.302549832533357\nAccuracy: 0.10684523809523809\nIteration 160 - cost: 2.302544272775136\nAccuracy: 0.1075\nIteration 170 - cost: 2.302538716053822\nAccuracy: 0.10809523809523809\nIteration 180 - cost: 2.3025331641765945\nAccuracy: 0.10851190476190477\nIteration 190 - cost: 2.302527614589447\nAccuracy: 0.10904761904761905\nIteration 200 - cost: 2.3025220660937937\nAccuracy: 0.10928571428571429\nIteration 210 - cost: 2.302516522069295\nAccuracy: 0.10976190476190475\nIteration 220 - cost: 2.3025109798298047\nAccuracy: 0.11002976190476191\nIteration 230 - cost: 2.302505439669589\nAccuracy: 0.11044642857142857\nIteration 240 - cost: 2.3024999007981393\nAccuracy: 0.11086309523809523\n0.10845238095238095\n{'layer_dims': [784, 50, 25, 15, 10], 'learning_rate': 0.001, 'num_iterations': 100}\nIteration 0 - cost: 2.302664817130649\nAccuracy: 0.11845238095238095\nIteration 10 - cost: 2.3026599452825827\nAccuracy: 0.11428571428571428\nIteration 20 - cost: 2.302655090116581\nAccuracy: 0.11205357142857143\nIteration 30 - cost: 2.302650252280837\nAccuracy: 0.10851190476190477\nIteration 40 - cost: 2.3026454341354428\nAccuracy: 0.1050297619047619\nIteration 50 - cost: 2.3026406374347284\nAccuracy: 0.10303571428571429\nIteration 60 - cost: 2.302635859158953\nAccuracy: 0.10113095238095238\nIteration 70 - cost: 2.3026310990203465\nAccuracy: 0.09922619047619048\nIteration 80 - cost: 2.30262635763441\nAccuracy: 0.09791666666666667\nIteration 90 - cost: 2.3026216313085635\nAccuracy: 0.09648809523809523\n0.09142857142857143\n","output_type":"stream"}]},{"cell_type":"code","source":"# find best accuracy\naccuracies = [result[2] for result in results]\nbest_accuracy_index = np.argmax(accuracies)\n\n# find best params\nbest_params = results[best_accuracy_index][1]","metadata":{"execution":{"iopub.status.busy":"2024-06-27T20:24:50.698431Z","iopub.execute_input":"2024-06-27T20:24:50.699239Z","iopub.status.idle":"2024-06-27T20:24:50.714776Z","shell.execute_reply.started":"2024-06-27T20:24:50.699207Z","shell.execute_reply":"2024-06-27T20:24:50.713868Z"},"trusted":true},"execution_count":58,"outputs":[{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"[{'layer_dims': [784, 20, 15, 10],\n  'learning_rate': 0.01,\n  'num_iterations': 250},\n {'W1': array([[-0.00529797,  0.00358079, -0.00786706, ..., -0.00507985,\n           0.0134123 ,  0.01558699],\n         [ 0.00394906, -0.00359972, -0.0120633 , ..., -0.01045295,\n          -0.00262962,  0.00068512],\n         [-0.0043093 ,  0.00205818, -0.00508573, ...,  0.00975014,\n          -0.0050386 ,  0.01956081],\n         ...,\n         [-0.00288385,  0.00347935,  0.00248177, ...,  0.00336121,\n           0.00828989, -0.01997622],\n         [-0.01271359,  0.00112289, -0.00665145, ...,  0.00611287,\n           0.00077539, -0.00151151],\n         [ 0.00052352, -0.02311181,  0.01916934, ...,  0.00349969,\n           0.01077255,  0.00146282]]),\n  'b1': array([[-1.22517415e-04],\n         [-5.56348217e-05],\n         [ 1.79088336e-04],\n         [ 6.18189536e-05],\n         [ 4.05377470e-07],\n         [ 4.59168366e-05],\n         [ 3.03285404e-05],\n         [ 2.78832591e-05],\n         [-4.30225752e-06],\n         [ 3.71460961e-05],\n         [ 3.98451775e-05],\n         [ 2.17786984e-05],\n         [ 3.90006993e-05],\n         [ 1.24439714e-04],\n         [-9.23195022e-05],\n         [ 4.15119196e-05],\n         [ 3.83468410e-05],\n         [ 3.95478050e-05],\n         [ 6.23816671e-05],\n         [ 2.67360026e-05]]),\n  'W2': array([[-7.93417296e-03,  7.46249527e-03, -2.80711299e-02,\n           1.35958078e-02,  1.88633066e-02, -5.08456100e-03,\n          -1.16480162e-02,  2.90144672e-02,  3.24530170e-02,\n          -7.94764807e-03,  2.43155954e-02,  5.85135949e-02,\n           2.15087036e-02,  1.58708417e-01,  5.53578772e-03,\n           5.49942177e-02,  7.67244897e-02, -8.38206175e-02,\n          -2.41790781e-02, -1.13489238e-02],\n         [-4.14110516e-02, -5.81725514e-02, -7.28241358e-02,\n          -2.80498038e-02,  1.08241802e-02,  3.92516550e-02,\n           8.49394620e-03, -1.07534371e-02,  3.11409281e-02,\n           4.83910171e-02,  4.13667595e-02, -9.60618213e-03,\n          -2.07999686e-02,  5.94064920e-03,  1.94833776e-02,\n          -4.60351496e-02,  3.19181183e-02,  5.48621190e-02,\n          -6.80684973e-03,  1.02122197e-01],\n         [-1.42667741e-02, -1.48769036e-02,  9.03573803e-03,\n           2.70957063e-04,  7.86614101e-03,  2.13025748e-02,\n           3.01424556e-02, -1.02904842e-03,  4.33165459e-03,\n           1.88046913e-02,  1.23316851e-02, -1.21121522e-02,\n           7.35135210e-03, -1.13089956e-02, -2.71598979e-03,\n          -4.11595494e-03, -2.69608004e-02,  1.16274193e-02,\n          -2.66532603e-03,  2.57454668e-02],\n         [-3.72386665e-02, -9.48756069e-03,  6.86920502e-02,\n          -3.53398430e-02,  1.39433856e-02, -7.97435007e-02,\n          -4.16857221e-02, -1.16195130e-02,  1.68657208e-02,\n           6.75052613e-02,  1.60588407e-01,  3.04657457e-02,\n           2.74226645e-03,  4.23399095e-02, -2.16749784e-02,\n          -2.57446500e-02,  4.09982404e-03,  1.88853685e-02,\n          -9.85861711e-03, -1.69821469e-02],\n         [ 9.48896107e-03, -3.94469727e-02, -3.08294745e-02,\n          -7.38268312e-02, -3.22173435e-03,  3.12149879e-02,\n          -2.32332812e-02,  3.89541342e-02, -3.80410392e-02,\n          -1.62256218e-03,  2.01337636e-02, -2.00238194e-02,\n          -2.25985194e-02,  6.12359432e-02,  1.16077659e-01,\n          -4.31899751e-02,  1.32385447e-02,  8.72400494e-02,\n           8.22376590e-03, -2.38569083e-02],\n         [ 1.10570128e-01,  8.14877453e-02,  7.73979416e-02,\n           3.75495189e-02, -1.46710171e-02, -9.95021196e-02,\n          -6.06198239e-02, -2.91120446e-02, -6.86313763e-03,\n          -5.20538495e-02, -3.92133157e-02, -2.32046309e-02,\n           7.11440013e-02, -3.12792070e-02,  7.59790749e-02,\n           6.19059597e-02, -6.53184815e-03,  4.71444191e-02,\n           1.01407979e-01, -4.84995569e-02],\n         [ 4.28281444e-02,  2.69078548e-02, -3.46685684e-02,\n          -5.93540733e-03, -9.65304829e-03, -1.69964581e-02,\n          -1.36468027e-02, -1.81735747e-02,  1.27467793e-02,\n           6.72619991e-03, -1.65589533e-02, -1.16778598e-02,\n           6.19648325e-02, -1.04651154e-02, -1.12517218e-02,\n           2.92929176e-02,  2.13439115e-02, -9.10239576e-03,\n           6.26872149e-02, -2.25172391e-02],\n         [ 9.20798408e-02,  4.34041444e-02, -4.76481725e-02,\n           7.62141993e-02, -5.22298695e-03,  1.36836393e-01,\n           6.03365616e-02, -1.35147763e-02,  3.40335914e-02,\n          -5.03688977e-02,  7.71222783e-02, -3.17410510e-02,\n          -4.25125828e-02, -4.65885792e-02,  5.91763724e-02,\n           5.21007016e-03,  4.90739979e-02,  1.89531716e-02,\n          -1.07702622e-01, -8.75812601e-02],\n         [ 1.65683741e-02,  2.93249613e-02, -3.78727922e-02,\n          -3.19269420e-02, -2.50677257e-03, -8.24613409e-04,\n           1.98838544e-02,  2.29044454e-02, -2.76206816e-02,\n          -1.00113549e-03, -1.17506034e-02, -4.96101797e-03,\n          -3.19916020e-02,  1.59394353e-02,  7.47134147e-02,\n          -6.01982874e-02, -2.64462027e-02,  5.66837662e-02,\n           1.02591387e-02,  2.83539579e-02],\n         [ 1.71872418e-02,  9.67189768e-03, -5.95365917e-03,\n          -9.81700794e-03,  6.23863440e-03, -2.35727038e-03,\n           1.90112663e-02,  7.56584098e-03, -1.74833369e-02,\n          -1.44827435e-02, -1.13352476e-02, -8.48848472e-03,\n          -1.32948792e-03,  7.37800809e-03, -1.30908462e-02,\n          -1.37193565e-02, -1.10543139e-02, -5.37225223e-03,\n           4.20640624e-03, -1.09309154e-02],\n         [ 3.72610575e-02, -3.16888026e-02,  1.50097105e-02,\n          -4.64684748e-02, -1.56559339e-02,  3.18334920e-02,\n           6.78816309e-03, -3.72165057e-02,  1.42042577e-01,\n           1.78770682e-02, -2.91007951e-02, -1.94050126e-02,\n          -5.49201220e-03, -3.02305896e-02,  5.99321390e-02,\n           5.87182612e-02,  1.09591921e-01, -5.28226371e-03,\n          -2.23903014e-02,  9.58954357e-02],\n         [-1.32134637e-02, -3.51157683e-03,  8.46319522e-02,\n           4.67434200e-02, -7.23888450e-03, -2.63842827e-02,\n           9.04962803e-04,  1.24337000e-03, -2.92573239e-02,\n           4.00042652e-03, -4.15223167e-02, -1.77468934e-02,\n           8.06963239e-03, -3.33627896e-02, -3.61303007e-02,\n           1.97906523e-05, -2.69210676e-02,  2.10590957e-02,\n           1.55082548e-02,  2.53530887e-02],\n         [-4.97173257e-03,  5.29759819e-03,  1.34590002e-01,\n           9.69452647e-02,  5.92427316e-03,  1.02072585e-01,\n           3.77511753e-02,  5.47283914e-02, -3.19114013e-02,\n           3.35226126e-02, -5.14094599e-03,  2.04480748e-02,\n          -4.93293944e-02, -3.81443439e-04,  1.81685914e-02,\n          -2.90845301e-02, -3.31382541e-02,  2.78278009e-02,\n          -2.26826071e-03,  7.11219234e-02],\n         [-1.66623687e-02, -1.89930973e-02, -3.83237478e-02,\n          -1.31815793e-02, -9.26118667e-03,  9.50412082e-03,\n           1.33031421e-02, -9.79854249e-03,  3.20356747e-02,\n           3.03522319e-02,  1.33955968e-02,  5.13081394e-03,\n           1.15881223e-02,  5.31449277e-03,  4.87101579e-03,\n           1.29167185e-03,  1.30068102e-02,  2.63230828e-02,\n           1.97353106e-02,  4.21038563e-02],\n         [ 2.36185954e-02,  2.38995958e-02,  1.70695848e-02,\n           3.68195140e-03, -8.41166674e-03,  1.83720748e-02,\n           1.76868014e-03,  9.72569058e-03, -4.16290132e-03,\n          -9.74911735e-03, -4.83134491e-03,  1.88255890e-02,\n           1.83418825e-02, -8.90285587e-03, -1.27419459e-02,\n           1.98213649e-02, -6.35997516e-03, -2.20428963e-02,\n           4.90372695e-02, -2.35994861e-02]]),\n  'b2': array([[ 8.77377475e-04],\n         [ 3.76642302e-04],\n         [ 1.27091110e-04],\n         [ 6.24429371e-04],\n         [ 3.29847600e-04],\n         [ 8.36414582e-04],\n         [ 5.21899865e-04],\n         [-1.02408210e-03],\n         [-7.35906570e-05],\n         [-3.72412581e-05],\n         [-8.12337106e-04],\n         [ 6.14836373e-04],\n         [ 1.30793796e-03],\n         [ 7.50515189e-06],\n         [ 4.73570462e-04]]),\n  'W3': array([[-0.06988951,  0.09088928,  0.03065142, -0.07596622, -0.00747833,\n          -0.11457646, -0.02252464, -0.04037861,  0.04197654,  0.00580306,\n           0.13322024, -0.02899452,  0.00983992,  0.05391348, -0.01054361],\n         [ 0.012686  , -0.04096911, -0.00511464, -0.0477799 , -0.03739152,\n           0.16824189,  0.10059074, -0.08719839, -0.03336462, -0.00290252,\n          -0.0816658 , -0.00283596, -0.13596229,  0.02148538,  0.04296468],\n         [ 0.05200487, -0.05785951, -0.01406279, -0.05121277, -0.0102766 ,\n           0.07182551,  0.01585692,  0.05572021, -0.06715979, -0.00752664,\n           0.13152116, -0.00422923, -0.05680851, -0.02270909, -0.01856293],\n         [ 0.0067452 , -0.0814231 , -0.0054731 , -0.05721191, -0.06311369,\n          -0.00902566, -0.02209784,  0.21264497, -0.0136867 ,  0.00610033,\n          -0.03092071, -0.01418867,  0.02678161, -0.02403325,  0.01767896],\n         [ 0.12519311, -0.05317738, -0.01711062,  0.10649229, -0.00383073,\n          -0.04978963, -0.02118162, -0.13656664, -0.00225944, -0.0022451 ,\n          -0.03886848, -0.03004235,  0.03949261, -0.01593899,  0.00310859],\n         [ 0.00619654,  0.03226809,  0.01964663, -0.07497735,  0.10786004,\n          -0.12166807, -0.01609791,  0.0776698 ,  0.03489002,  0.00225283,\n          -0.03681511, -0.00608263,  0.06674711, -0.00130634,  0.01416629],\n         [ 0.0515789 ,  0.08529336, -0.00793067,  0.08452165,  0.01695113,\n          -0.06934383, -0.01170332,  0.00160792, -0.03335612,  0.01031069,\n           0.07447074, -0.03936039, -0.17778245,  0.0444025 ,  0.00554423],\n         [-0.03647732,  0.04933543,  0.01190389, -0.03711628, -0.11006137,\n           0.06123102, -0.02186659, -0.12658097, -0.02757836, -0.00253191,\n          -0.06972615,  0.11192586,  0.12330461,  0.0208649 ,  0.02376523],\n         [-0.03494143, -0.03828974, -0.02486137, -0.02254587,  0.0988256 ,\n           0.07596068, -0.03462233,  0.03122377,  0.09067515, -0.01911933,\n          -0.06221034, -0.01285047, -0.05102017, -0.0150823 , -0.04402024],\n         [-0.10510639, -0.00096059,  0.02431278,  0.15258198, -0.02912125,\n          -0.00746883, -0.01839409, -0.04197729, -0.03536733,  0.00589244,\n          -0.04760231,  0.02287812,  0.06233515, -0.00435899,  0.00580658]]),\n  'b3': array([[-0.02516598],\n         [ 0.01670505],\n         [-0.00930278],\n         [-0.00703286],\n         [ 0.01494981],\n         [ 0.00639476],\n         [ 0.00093592],\n         [ 0.01043029],\n         [-0.01135482],\n         [ 0.0034406 ]])},\n 0.9158333333333334]"},"metadata":{}}]},{"cell_type":"code","source":"#make predictions on test set\ntest_preds = predict(test, best_params)\n\n# create submission dataframe\nsubmission = pd.DataFrame(data=\n                          {'ImageId':np.arange(1, test.shape[1]+1),\n                          'Label':test_preds})\n\n# export dataframe to csv\nsubmission.to_csv('submission.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]}]}